Traceback (most recent call last):
  File "/Users/thomasryde/Documents/University/Kandidat/3. Semester/Machine Learning Operations/mlops/src/models/train_model.py", line 87, in <module>
    val_acc = val_correct / val_total
ZeroDivisionError: division by zero
USING DEVICE cpu
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1            [-1, 256, 8, 8]          49,408
    PatchEmbedding-2              [-1, 65, 256]               0
         LayerNorm-3              [-1, 65, 256]             512
            Linear-4              [-1, 65, 768]         196,608
           Dropout-5            [-1, 1, 65, 65]               0
            Linear-6              [-1, 65, 256]          65,792
           Dropout-7              [-1, 65, 256]               0
MultiHeadAttention-8              [-1, 65, 256]               0
           Dropout-9              [-1, 65, 256]               0
      ResidualAdd-10              [-1, 65, 256]               0
        LayerNorm-11              [-1, 65, 256]             512
           Linear-12              [-1, 65, 256]          65,792
             GELU-13              [-1, 65, 256]               0
          Dropout-14              [-1, 65, 256]               0
           Linear-15              [-1, 65, 256]          65,792
          Dropout-16              [-1, 65, 256]               0
          Dropout-17              [-1, 65, 256]               0
      ResidualAdd-18              [-1, 65, 256]               0
        LayerNorm-19              [-1, 65, 256]             512
           Linear-20              [-1, 65, 768]         196,608
          Dropout-21            [-1, 1, 65, 65]               0
           Linear-22              [-1, 65, 256]          65,792
          Dropout-23              [-1, 65, 256]               0
MultiHeadAttention-24              [-1, 65, 256]               0
          Dropout-25              [-1, 65, 256]               0
      ResidualAdd-26              [-1, 65, 256]               0
        LayerNorm-27              [-1, 65, 256]             512
           Linear-28              [-1, 65, 256]          65,792
             GELU-29              [-1, 65, 256]               0
          Dropout-30              [-1, 65, 256]               0
           Linear-31              [-1, 65, 256]          65,792
          Dropout-32              [-1, 65, 256]               0
          Dropout-33              [-1, 65, 256]               0
      ResidualAdd-34              [-1, 65, 256]               0
        LayerNorm-35              [-1, 65, 256]             512
           Linear-36              [-1, 65, 768]         196,608
          Dropout-37            [-1, 1, 65, 65]               0
           Linear-38              [-1, 65, 256]          65,792
          Dropout-39              [-1, 65, 256]               0
MultiHeadAttention-40              [-1, 65, 256]               0
          Dropout-41              [-1, 65, 256]               0
      ResidualAdd-42              [-1, 65, 256]               0
        LayerNorm-43              [-1, 65, 256]             512
           Linear-44              [-1, 65, 256]          65,792
             GELU-45              [-1, 65, 256]               0
          Dropout-46              [-1, 65, 256]               0
           Linear-47              [-1, 65, 256]          65,792
          Dropout-48              [-1, 65, 256]               0
          Dropout-49              [-1, 65, 256]               0
      ResidualAdd-50              [-1, 65, 256]               0
        LayerNorm-51              [-1, 65, 256]             512
           Linear-52              [-1, 65, 768]         196,608
          Dropout-53            [-1, 1, 65, 65]               0
           Linear-54              [-1, 65, 256]          65,792
          Dropout-55              [-1, 65, 256]               0
MultiHeadAttention-56              [-1, 65, 256]               0
          Dropout-57              [-1, 65, 256]               0
      ResidualAdd-58              [-1, 65, 256]               0
        LayerNorm-59              [-1, 65, 256]             512
           Linear-60              [-1, 65, 256]          65,792
             GELU-61              [-1, 65, 256]               0
          Dropout-62              [-1, 65, 256]               0
           Linear-63              [-1, 65, 256]          65,792
          Dropout-64              [-1, 65, 256]               0
          Dropout-65              [-1, 65, 256]               0
      ResidualAdd-66              [-1, 65, 256]               0
        LayerNorm-67              [-1, 65, 256]             512
           Linear-68              [-1, 65, 768]         196,608
          Dropout-69            [-1, 1, 65, 65]               0
           Linear-70              [-1, 65, 256]          65,792
          Dropout-71              [-1, 65, 256]               0
MultiHeadAttention-72              [-1, 65, 256]               0
          Dropout-73              [-1, 65, 256]               0
      ResidualAdd-74              [-1, 65, 256]               0
        LayerNorm-75              [-1, 65, 256]             512
           Linear-76              [-1, 65, 256]          65,792
             GELU-77              [-1, 65, 256]               0
          Dropout-78              [-1, 65, 256]               0
           Linear-79              [-1, 65, 256]          65,792
          Dropout-80              [-1, 65, 256]               0
          Dropout-81              [-1, 65, 256]               0
      ResidualAdd-82              [-1, 65, 256]               0
        LayerNorm-83              [-1, 65, 256]             512
           Linear-84              [-1, 65, 768]         196,608
          Dropout-85            [-1, 1, 65, 65]               0
           Linear-86              [-1, 65, 256]          65,792
          Dropout-87              [-1, 65, 256]               0
MultiHeadAttention-88              [-1, 65, 256]               0
          Dropout-89              [-1, 65, 256]               0
      ResidualAdd-90              [-1, 65, 256]               0
        LayerNorm-91              [-1, 65, 256]             512
           Linear-92              [-1, 65, 256]          65,792
             GELU-93              [-1, 65, 256]               0
          Dropout-94              [-1, 65, 256]               0
           Linear-95              [-1, 65, 256]          65,792
          Dropout-96              [-1, 65, 256]               0
          Dropout-97              [-1, 65, 256]               0
      ResidualAdd-98              [-1, 65, 256]               0
        LayerNorm-99              [-1, 65, 256]             512
          Linear-100              [-1, 65, 768]         196,608
         Dropout-101            [-1, 1, 65, 65]               0
          Linear-102              [-1, 65, 256]          65,792
         Dropout-103              [-1, 65, 256]               0
MultiHeadAttention-104              [-1, 65, 256]               0
         Dropout-105              [-1, 65, 256]               0
     ResidualAdd-106              [-1, 65, 256]               0
       LayerNorm-107              [-1, 65, 256]             512
          Linear-108              [-1, 65, 256]          65,792
            GELU-109              [-1, 65, 256]               0
         Dropout-110              [-1, 65, 256]               0
          Linear-111              [-1, 65, 256]          65,792
         Dropout-112              [-1, 65, 256]               0
         Dropout-113              [-1, 65, 256]               0
     ResidualAdd-114              [-1, 65, 256]               0
       LayerNorm-115              [-1, 65, 256]             512
          Linear-116              [-1, 65, 768]         196,608
         Dropout-117            [-1, 1, 65, 65]               0
          Linear-118              [-1, 65, 256]          65,792
         Dropout-119              [-1, 65, 256]               0
MultiHeadAttention-120              [-1, 65, 256]               0
         Dropout-121              [-1, 65, 256]               0
     ResidualAdd-122              [-1, 65, 256]               0
       LayerNorm-123              [-1, 65, 256]             512
          Linear-124              [-1, 65, 256]          65,792
            GELU-125              [-1, 65, 256]               0
         Dropout-126              [-1, 65, 256]               0
          Linear-127              [-1, 65, 256]          65,792
         Dropout-128              [-1, 65, 256]               0
         Dropout-129              [-1, 65, 256]               0
     ResidualAdd-130              [-1, 65, 256]               0
       LayerNorm-131              [-1, 65, 256]             512
          Linear-132              [-1, 65, 768]         196,608
         Dropout-133            [-1, 1, 65, 65]               0
          Linear-134              [-1, 65, 256]          65,792
         Dropout-135              [-1, 65, 256]               0
MultiHeadAttention-136              [-1, 65, 256]               0
         Dropout-137              [-1, 65, 256]               0
     ResidualAdd-138              [-1, 65, 256]               0
       LayerNorm-139              [-1, 65, 256]             512
          Linear-140              [-1, 65, 256]          65,792
            GELU-141              [-1, 65, 256]               0
         Dropout-142              [-1, 65, 256]               0
          Linear-143              [-1, 65, 256]          65,792
         Dropout-144              [-1, 65, 256]               0
         Dropout-145              [-1, 65, 256]               0
     ResidualAdd-146              [-1, 65, 256]               0
       LayerNorm-147              [-1, 65, 256]             512
          Linear-148              [-1, 65, 768]         196,608
         Dropout-149            [-1, 1, 65, 65]               0
          Linear-150              [-1, 65, 256]          65,792
         Dropout-151              [-1, 65, 256]               0
MultiHeadAttention-152              [-1, 65, 256]               0
         Dropout-153              [-1, 65, 256]               0
     ResidualAdd-154              [-1, 65, 256]               0
       LayerNorm-155              [-1, 65, 256]             512
          Linear-156              [-1, 65, 256]          65,792
            GELU-157              [-1, 65, 256]               0
         Dropout-158              [-1, 65, 256]               0
          Linear-159              [-1, 65, 256]          65,792
         Dropout-160              [-1, 65, 256]               0
         Dropout-161              [-1, 65, 256]               0
     ResidualAdd-162              [-1, 65, 256]               0
       LayerNorm-163              [-1, 65, 256]             512
          Linear-164              [-1, 65, 768]         196,608
         Dropout-165            [-1, 1, 65, 65]               0
          Linear-166              [-1, 65, 256]          65,792
         Dropout-167              [-1, 65, 256]               0
MultiHeadAttention-168              [-1, 65, 256]               0
         Dropout-169              [-1, 65, 256]               0
     ResidualAdd-170              [-1, 65, 256]               0
       LayerNorm-171              [-1, 65, 256]             512
          Linear-172              [-1, 65, 256]          65,792
            GELU-173              [-1, 65, 256]               0
         Dropout-174              [-1, 65, 256]               0
          Linear-175              [-1, 65, 256]          65,792
         Dropout-176              [-1, 65, 256]               0
         Dropout-177              [-1, 65, 256]               0
     ResidualAdd-178              [-1, 65, 256]               0
       LayerNorm-179              [-1, 65, 256]             512
          Linear-180              [-1, 65, 768]         196,608
         Dropout-181            [-1, 1, 65, 65]               0
          Linear-182              [-1, 65, 256]          65,792
         Dropout-183              [-1, 65, 256]               0
MultiHeadAttention-184              [-1, 65, 256]               0
         Dropout-185              [-1, 65, 256]               0
     ResidualAdd-186              [-1, 65, 256]               0
       LayerNorm-187              [-1, 65, 256]             512
          Linear-188              [-1, 65, 256]          65,792
            GELU-189              [-1, 65, 256]               0
         Dropout-190              [-1, 65, 256]               0
          Linear-191              [-1, 65, 256]          65,792
         Dropout-192              [-1, 65, 256]               0
         Dropout-193              [-1, 65, 256]               0
     ResidualAdd-194              [-1, 65, 256]               0
TransformerEncoder-195              [-1, 65, 256]               0
VisionTransformer-196              [-1, 65, 256]               0
       LayerNorm-197                  [-1, 256]             512
          Linear-198                    [-1, 2]             514
ClassificationHead-199                    [-1, 2]               0
================================================================
Total params: 4,790,530
Trainable params: 4,790,530
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.05
Forward/backward pass size (MB): 26.79
Params size (MB): 18.27
Estimated Total Size (MB): 45.11
----------------------------------------------------------------
========= EPOCH 0 =========